{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073f64cf",
   "metadata": {},
   "source": [
    "# General Approach and Requirements\n",
    "\n",
    "The highest scoring model with the smallest required hardware to train and run was the FPN2 model [A11], MMN [A9] or MFaster-RCNN[A.14]. These all required a GTX 1080 TI, which can actually be run on a few froup member's hardware. I will be experimenting with the MFaster-RCNN detection architecture with an FPN integrated to handle the different scales of objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e38489",
   "metadata": {},
   "source": [
    "# MFaster-RCNN with FPN2\n",
    "Following are the packages that we need for the MFaster architecture with an FPN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CUDA_LAUNCH_BLOCKING=1\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.transforms as T\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import sys\n",
    "import utils\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import traceback\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83167f5",
   "metadata": {},
   "source": [
    "# Capture the Dataset for Preprocessing\n",
    "Read in all of the image files into a list, as well as their corresponding annotations. The dataset also has a getter for the lenght of the dataset for iterative purposes later along with a getitem function for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisDroneDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.image_files = sorted(os.listdir(root_dir))\n",
    "        self.transforms = transforms\n",
    "        self.annotations = self._load_annotations()\n",
    "        self.classes = ['background', 'predestrian', 'people', 'car', 'van', 'bus', 'truck', 'tricycle', 'awning-tricycle', 'bicycle', 'motorcycle']\n",
    "\n",
    "    def _load_annotations(self):\n",
    "        annotations = {}\n",
    "        for imgName in self.image_files:\n",
    "            annotation_name = imgName.replace('.jpg', '.txt')\n",
    "            annotation_path = os.path.join(self.annotation_dir, annotation_name)\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            if os.path.exists(annotation_path):\n",
    "                with open(annotation_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            x, y, w, h, score, categoryID, truncation, occlusion = map(int, line.strip().split(',')[:8])\n",
    "\n",
    "                            if 1 <= categoryID <= 10:\n",
    "                                boxes.append([x, y, x+w, y+h])\n",
    "                                labels.append(categoryID)\n",
    "                            \n",
    "                        except ValueError as e:\n",
    "                            print(f\"Error parsing line in {annotation_path}: {line.strip()} - {e}\")\n",
    "            annotations[imgName] = {'boxes': boxes, 'labels': labels}\n",
    "            \n",
    "        return annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgName = self.image_files[idx]\n",
    "        imgPath = os.path.join(self.root_dir, imgName)\n",
    "        annotationData = self.annotations[imgName]\n",
    "\n",
    "        img = Image.open(imgPath).convert(\"RGB\")\n",
    "        boxes = torch.as_tensor(annotationData['boxes'], dtype=torch.float32) \n",
    "        labels = torch.as_tensor(annotationData['labels'], dtype=torch.int64) \n",
    "        \n",
    "        # with open(annotation_path, 'r') as f:\n",
    "        #     for line in f:\n",
    "        #         x, y, w, h, score, categoryID, truncation, occlusion = map(int, line.strip().split(',')[:8]) # Assuming standard VisDrone annotation format\n",
    "        #         if not (1 < categoryID <= 10):\n",
    "        #             continue\n",
    "        #         boxes.append([x, y, x+w, y+h])\n",
    "        #         labels.append(categoryID + 1) # Assuming the 6th value is the class label\n",
    "\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"iamge_id\": torch.tensor([idx])\n",
    "        }\n",
    "        \n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082c6ac",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Resize and normalize the dataset with flips and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9578f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50688c22",
   "metadata": {},
   "source": [
    "# Load the dataset with dataloaders\n",
    "Create custom dataset with the augmented data appended to the original dataset using the data loaders from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5949ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e6b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, dataLoader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    metricLogger = utils.MetricLogger(delimiter=\" \")\n",
    "    metricLogger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = f'Epoch: [{epoch}]'\n",
    "    \n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmupFactor = 1./100\n",
    "        warmupIter = min(len(dataLoader) - 1, 100)\n",
    "        \n",
    "        def lr_lambda(step):\n",
    "            if step < warmupIter:\n",
    "                alpha = float(step) / warmupIter\n",
    "                return warmupFactor * (1 - alpha) + alpha\n",
    "            return 1\n",
    "        \n",
    "        lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "        \n",
    "    for i, (images, targets) in enumerate(metricLogger.log_every(dataLoader, print_freq, header)):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "        lossDict = model(images, targets)\n",
    "        losses = sum(loss for loss in lossDict.values())\n",
    "        \n",
    "        lossDictReduced = utils.reduce_dict(lossDict)\n",
    "        losses_reduced = sum(loss for loss in lossDictReduced.values())\n",
    "        \n",
    "        lossValue = losses_reduced.item()\n",
    "        \n",
    "        if not torch.isfinite(lossValue):\n",
    "            print(f\"Loss is {lossValue}, stopping training\")\n",
    "            print(lossDictReduced)\n",
    "            sys.exit(1)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "        metricLogger.update(loss=losses_reduced, **lossDictReduced)\n",
    "        metricLogger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "    return metricLogger\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    metricLogger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "    \n",
    "    coco = convertToCOCO(data_loader.dataset)\n",
    "    iou_types = [\"bbox\"]\n",
    "    cocoEvaluator = cocoEvaluator(coco, iou_types)\n",
    "    \n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in outputs]\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        cocoEvaluator.update(res)\n",
    "        \n",
    "    cocoEvaluator.synchronize_between_processes()\n",
    "    \n",
    "    cocoEvaluator.accumulate()\n",
    "    cocoEvaluator.summarize()\n",
    "    \n",
    "    return cocoEvaluator\n",
    "\n",
    "class CocoEvaluator:\n",
    "    def __init__(self, cocoGt, iouTypes):\n",
    "        if not isinstance(iouTypes, (list, tuple)):\n",
    "            raise TypeError(f\"iou_types must be a list or tuple of strings, got {iouTypes}\")\n",
    "        allowed_iou_types = (\"bbox\", \"segm\")\n",
    "        for iou_type in iouTypes:\n",
    "            if iou_type not in allowed_iou_types:\n",
    "                raise ValueError(f\"iou_type: {iou_type} not in {allowed_iou_types}\")\n",
    "        self.cocoGt = cocoGt\n",
    "        self.iouTypes = iouTypes\n",
    "        self.cocoEval = {}\n",
    "        \n",
    "    def update(self, predictions):\n",
    "        imgIds = list(np.unique(list(predictions.keys())))\n",
    "        self.imgIds.extend(imgIds)\n",
    "        \n",
    "        for iou_type in self.iouTypes:\n",
    "            if len(self.cocoEval) == 0:\n",
    "                self.cocoEval[iou_type] = COCOeval(self.cocoGt, _create_coco_results(self.coco_gt, predictions, iou_type), iou_type)\n",
    "            else:\n",
    "                cocoDt = _create_coco_results(self.cocoGt, predictions, iou_type)\n",
    "                self.cocoEval[iou_type].cocoDt = self.cocoEval[iou_type].cocoGt.loadRes(cocoDt)\n",
    "            \n",
    "    def synchronize_between_processes(self):\n",
    "        pass\n",
    "    \n",
    "    def accumulate(self):\n",
    "        for coco_eval in self.cocoEval.values():\n",
    "            coco_eval.accumulate()\n",
    "            \n",
    "    def summarize(self):\n",
    "        for iouType, cocoEval in self.coco_eval.items():\n",
    "            print(f\"IoU metric: {iouType}\")\n",
    "            cocoEval.summarize()\n",
    "            \n",
    "    @property\n",
    "    def results(self):\n",
    "        return {iouType: cocoEval.stats.tolist() for iouType, cocoEval in self.cocoEval.items()}\n",
    "            \n",
    "     \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d37c5",
   "metadata": {},
   "source": [
    "# Make Results in COCO format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741abdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_coco_results(cocoGt, predictions, iouType): \n",
    "    results = []\n",
    "    for image_id, prediction in predictions.items():\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "\n",
    "        boxes = prediction[\"boxes\"].tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        coco_predictions = []\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            coco_predictions.append(\n",
    "                {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"bbox\": [box[0], box[1], box[2] - box[0], box[3] - box[1]], # Convert to [x, y, w, h]\n",
    "                    \"score\": score,\n",
    "                    \"category_id\": int(label),\n",
    "                }\n",
    "            )\n",
    "        results.extend(coco_predictions)\n",
    "    return results\n",
    "\n",
    "def convertToCOCO(dataset):\n",
    "    coco = COCO()\n",
    "    coco.dataset = {\n",
    "        \"info\": {\"description\": \"VisDrone 2019 Dataset\"},\n",
    "        \"licenses\": [{\"id\": 1, \"name\": \"Unknown\", \"url\": \"\"}],\n",
    "        \"categories\": [{\"id\": i + 1, \"name\": name} for i, name in enumerate(dataset.classes[1:])], # Exclude 'ignored region'\n",
    "        \"images\": [],\n",
    "        \"annotations\": []\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "    for i in range(len(dataset)):\n",
    "        img_name = dataset.image_files[i]\n",
    "        _, target = dataset[i]\n",
    "        image_info = {\"id\": i, \"file_name\": img_name, \"width\": 0, \"height\": 0} # Actual width and height are not used in evaluation\n",
    "        coco.dataset[\"images\"].append(image_info)\n",
    "\n",
    "        for j in range(len(target[\"boxes\"])):\n",
    "            bbox = target[\"boxes\"][j].tolist()\n",
    "            label = target[\"labels\"][j].item()\n",
    "            annotation = {\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": i,\n",
    "                \"category_id\": label,\n",
    "                \"bbox\": [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]], # Convert to [x, y, w, h]\n",
    "                \"area\": (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]),\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            coco.dataset[\"annotations\"].append(annotation)\n",
    "            annotation_id += 1\n",
    "\n",
    "    coco.createIndex()\n",
    "    return coco\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329a287",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33301708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad labels at index 0: tensor([ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  3,\n",
      "         5,  5,  5,  5,  5,  5,  5,  6,  5,  6,  5,  6,  6,  7,  5,  5,  5,  5,\n",
      "        11,  4,  5,  5,  5,  8,  5,  9,  5,  5,  6,  5,  5,  5,  3,  3,  4,  4,\n",
      "         3, 11,  3, 11,  3, 11, 11,  3,  3, 11,  9,  3,  6, 11, 11,  3,  3])\n",
      "Batch size: 4\n",
      "Image shape: [torch.Size([3, 1050, 1400]), torch.Size([3, 1078, 1916]), torch.Size([3, 1500, 2000]), torch.Size([3, 1078, 1916])]\n",
      "Targets: ({'boxes': tensor([[1358.,  279., 1379.,  303.],\n",
      "        [1319.,  268., 1351.,  292.],\n",
      "        [1332.,  245., 1344.,  260.],\n",
      "        [1315.,  247., 1326.,  257.],\n",
      "        [1311.,  246., 1319.,  257.],\n",
      "        [1300.,  239., 1306.,  245.],\n",
      "        [1292.,  252., 1299.,  261.],\n",
      "        [1154.,  410., 1167.,  424.],\n",
      "        [1169.,  422., 1190.,  439.],\n",
      "        [1181.,  423., 1201.,  445.],\n",
      "        [1155.,  475., 1180.,  498.],\n",
      "        [1135.,  469., 1156.,  488.],\n",
      "        [1112.,  466., 1128.,  484.],\n",
      "        [1085.,  544., 1109.,  569.],\n",
      "        [1065.,  542., 1089.,  569.],\n",
      "        [1014.,  575., 1030.,  596.],\n",
      "        [ 987.,  592., 1009.,  613.],\n",
      "        [1059.,  621., 1070.,  632.],\n",
      "        [1064.,  624., 1076.,  640.],\n",
      "        [ 990.,  630., 1022.,  659.],\n",
      "        [ 969.,  674., 1007.,  709.],\n",
      "        [ 821.,  791.,  855.,  828.],\n",
      "        [ 784.,  834.,  824.,  885.],\n",
      "        [ 759.,  917.,  812.,  972.],\n",
      "        [ 746.,  909.,  790.,  959.],\n",
      "        [ 680.,  935.,  723.,  984.],\n",
      "        [ 695.,  968.,  741., 1024.],\n",
      "        [ 654.,  836.,  665.,  845.],\n",
      "        [ 635.,  857.,  642.,  863.],\n",
      "        [ 646.,  836.,  656.,  846.],\n",
      "        [ 642.,  837.,  649.,  847.],\n",
      "        [ 630.,  857.,  639.,  862.],\n",
      "        [ 622.,  855.,  631.,  863.],\n",
      "        [ 635.,  838.,  644.,  844.]]), 'labels': tensor([ 6,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  5,  5,  3,\n",
      "        11,  6,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5]), 'iamge_id': tensor([5547])}, {'boxes': tensor([[7.4400e+02, 1.0270e+03, 7.9800e+02, 1.0770e+03],\n",
      "        [1.2000e+01, 3.4200e+02, 5.7000e+01, 3.6700e+02],\n",
      "        [9.2200e+02, 5.4900e+02, 9.9200e+02, 5.7900e+02],\n",
      "        [9.2000e+02, 7.4400e+02, 1.0060e+03, 7.8300e+02],\n",
      "        [9.1800e+02, 7.1300e+02, 1.0080e+03, 7.4900e+02],\n",
      "        [8.9000e+02, 7.9900e+02, 1.0020e+03, 8.4700e+02],\n",
      "        [8.0100e+02, 5.0900e+02, 8.4300e+02, 5.5800e+02],\n",
      "        [9.3000e+02, 4.9700e+02, 9.9800e+02, 5.2700e+02],\n",
      "        [9.2900e+02, 4.7900e+02, 9.9500e+02, 5.0600e+02],\n",
      "        [9.3700e+02, 4.2900e+02, 9.9500e+02, 4.6300e+02],\n",
      "        [9.7300e+02, 3.7600e+02, 1.0320e+03, 4.0200e+02],\n",
      "        [8.3100e+02, 4.6200e+02, 8.4500e+02, 4.8500e+02],\n",
      "        [8.2200e+02, 4.6100e+02, 8.3200e+02, 4.8700e+02],\n",
      "        [8.1800e+02, 3.3600e+02, 8.6500e+02, 3.5500e+02],\n",
      "        [8.3200e+02, 2.7300e+02, 8.7200e+02, 2.9100e+02],\n",
      "        [8.3900e+02, 2.5300e+02, 8.8300e+02, 2.6900e+02],\n",
      "        [8.4600e+02, 2.3200e+02, 8.9200e+02, 2.4800e+02],\n",
      "        [1.4630e+03, 4.2500e+02, 1.5230e+03, 4.5700e+02],\n",
      "        [1.4400e+03, 3.9600e+02, 1.4460e+03, 4.1200e+02],\n",
      "        [1.4400e+03, 3.9800e+02, 1.4510e+03, 4.1000e+02],\n",
      "        [1.4290e+03, 4.0100e+02, 1.4540e+03, 4.1700e+02],\n",
      "        [1.5300e+03, 3.8400e+02, 1.5510e+03, 3.9600e+02],\n",
      "        [1.5590e+03, 3.8400e+02, 1.5750e+03, 3.9900e+02],\n",
      "        [6.9600e+02, 9.3700e+02, 7.9800e+02, 9.8100e+02],\n",
      "        [1.0690e+03, 1.1400e+02, 1.1010e+03, 1.2800e+02],\n",
      "        [1.3780e+03, 3.5000e+01, 1.3940e+03, 4.8000e+01],\n",
      "        [1.3710e+03, 9.0000e+00, 1.4000e+03, 3.5000e+01],\n",
      "        [1.3510e+03, 1.0000e+00, 1.3820e+03, 2.7000e+01],\n",
      "        [1.3490e+03, 1.1700e+02, 1.3790e+03, 1.3500e+02],\n",
      "        [1.3310e+03, 1.0600e+02, 1.3640e+03, 1.2000e+02],\n",
      "        [1.3270e+03, 1.0000e+02, 1.3560e+03, 1.1400e+02],\n",
      "        [1.3230e+03, 9.0000e+01, 1.3510e+03, 1.0300e+02],\n",
      "        [1.3350e+03, 1.1300e+02, 1.3660e+03, 1.2500e+02],\n",
      "        [1.3130e+03, 8.8000e+01, 1.3420e+03, 1.0000e+02]]), 'labels': tensor([ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5, 11, 11,  5,  5,  5,  5,  5,\n",
      "         3,  3, 11, 11, 11,  5,  5,  5, 10, 10,  6,  6,  6,  6,  5,  5]), 'iamge_id': tensor([4635])}, {'boxes': tensor([[ 992.,   48., 1004.,   63.],\n",
      "        [1199.,   90., 1215.,  105.],\n",
      "        [1147.,  119., 1178.,  138.],\n",
      "        [ 950.,  163.,  965.,  184.],\n",
      "        [ 948.,  182.,  965.,  204.],\n",
      "        [ 944.,  199.,  965.,  224.],\n",
      "        [1245.,  223., 1266.,  244.],\n",
      "        [ 938.,  264.,  959.,  292.],\n",
      "        [ 933.,  283.,  952.,  310.],\n",
      "        [ 932.,  304.,  954.,  333.],\n",
      "        [ 921.,  394.,  944.,  421.],\n",
      "        [ 901.,  501.,  913.,  519.],\n",
      "        [ 903.,  510.,  955.,  567.],\n",
      "        [ 895.,  558.,  931.,  615.],\n",
      "        [ 892.,  618.,  927.,  674.],\n",
      "        [ 869.,  729.,  951.,  792.],\n",
      "        [ 869.,  787.,  910.,  868.],\n",
      "        [ 842.,  922.,  924., 1005.],\n",
      "        [ 835.,  994.,  915., 1092.],\n",
      "        [ 819., 1079.,  917., 1177.],\n",
      "        [ 799., 1201.,  905., 1329.],\n",
      "        [ 776., 1359.,  903., 1489.],\n",
      "        [1618., 1051., 1705., 1167.],\n",
      "        [1794., 1437., 1887., 1499.],\n",
      "        [1128., 1127., 1184., 1237.],\n",
      "        [1064.,  854., 1139.,  915.],\n",
      "        [1070.,  804., 1138.,  861.],\n",
      "        [1493.,  769., 1527.,  796.],\n",
      "        [1595.,  764., 1614.,  792.],\n",
      "        [1459.,  713., 1516.,  778.],\n",
      "        [1065.,  703., 1122.,  770.],\n",
      "        [1259.,  638., 1326.,  679.],\n",
      "        [1053.,  649., 1119.,  705.],\n",
      "        [1060.,  613., 1119.,  669.],\n",
      "        [1076.,  574., 1106.,  619.],\n",
      "        [1069.,  463., 1103.,  525.],\n",
      "        [1060.,  395., 1083.,  428.],\n",
      "        [1057.,  366., 1079.,  396.],\n",
      "        [1121.,  345., 1171.,  369.],\n",
      "        [1223.,  341., 1262.,  402.],\n",
      "        [1238.,  338., 1283.,  370.],\n",
      "        [1215.,  318., 1269.,  342.],\n",
      "        [1328.,  424., 1361.,  465.],\n",
      "        [1346.,  353., 1377.,  388.],\n",
      "        [1269.,  308., 1290.,  323.],\n",
      "        [1324.,  272., 1335.,  288.],\n",
      "        [1323.,  298., 1336.,  312.]]), 'labels': tensor([ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  3,  7,  5,  5,  5,  6,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  6,  5, 11, 11,  5,  5,  6,  5,  5,  6,  7,\n",
      "         5,  5,  5,  7,  5,  5,  6,  5, 11,  3, 11]), 'iamge_id': tensor([6089])}, {'boxes': tensor([[ 686.,  614.,  747.,  685.],\n",
      "        [ 787.,  663.,  832.,  730.],\n",
      "        [ 772.,  673.,  857.,  727.],\n",
      "        [ 875.,  620.,  933.,  710.],\n",
      "        [ 974.,  481., 1009.,  542.],\n",
      "        [ 970.,  440., 1019.,  565.],\n",
      "        [ 764.,  900.,  858., 1077.],\n",
      "        [1011.,  951., 1084., 1077.],\n",
      "        [1286.,  995., 1344., 1068.],\n",
      "        [1272.,  996., 1344., 1077.],\n",
      "        [1408.,  968., 1466., 1021.],\n",
      "        [1354.,  959., 1503., 1077.],\n",
      "        [1428.,  894., 1473.,  955.],\n",
      "        [1277.,  600., 1444.,  878.],\n",
      "        [ 724.,  591.,  873.,  674.],\n",
      "        [1122.,  489., 1153.,  533.],\n",
      "        [1122.,  471., 1152.,  526.],\n",
      "        [1121.,  475., 1152.,  536.],\n",
      "        [1068.,  158., 1142.,  277.],\n",
      "        [1156.,   83., 1217.,  161.],\n",
      "        [ 888.,  199.,  950.,  286.],\n",
      "        [ 912.,  138.,  959.,  222.],\n",
      "        [ 934.,  188.,  957.,  229.],\n",
      "        [ 930.,   98.,  948.,  126.],\n",
      "        [ 922.,   85.,  946.,  124.],\n",
      "        [ 924.,   88.,  948.,  128.]]), 'labels': tensor([11,  3, 11, 11,  3,  8,  9,  9,  3, 11,  3,  8,  3,  7,  8,  3,  3, 11,\n",
      "         7,  6,  9,  8,  3,  3,  3, 11]), 'iamge_id': tensor([1528])})\n",
      "Error in training loop: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([0]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1615801/4183841071.py\", line 28, in <module>\n",
      "    loss_dict = model(imgs, targets)\n",
      "                ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tow73/COMP9444/VisDroneInterpretaion/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tow73/COMP9444/VisDroneInterpretaion/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tow73/COMP9444/VisDroneInterpretaion/myenv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 67, in forward\n",
      "    torch._assert(\n",
      "  File \"/home/tow73/COMP9444/VisDroneInterpretaion/myenv/lib/python3.12/site-packages/torch/__init__.py\", line 2132, in _assert\n",
      "    assert condition, message\n",
      "           ^^^^^^^^^\n",
      "AssertionError: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([0]).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     37\u001b[39m         traceback.print_exc()\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mlosses\u001b[49m.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define paths\n",
    "    train_img_dir = \"VisDrone2019-DET-train/images\"\n",
    "    train_anno_dir = \"VisDrone2019-DET-train/annotations\"\n",
    "    val_img_dir = \"VisDrone2019-DET-val/images\"\n",
    "    val_anno_dir = \"VisDrone2019-DET-val/annotations\"\n",
    "\n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(train_img_dir) or not os.path.exists(train_anno_dir) or not os.path.exists(val_img_dir) or not os.path.exists(val_anno_dir):\n",
    "        print(\"Error: One or more image or annotation directories not found. Please adjust the paths.\")\n",
    "        return\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Define transforms\n",
    "    train_transforms = get_transform(train=True)\n",
    "    val_transforms = get_transform(train=False)\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = VisDroneDataset(train_img_dir, train_anno_dir, transforms=train_transforms)\n",
    "    val_dataset = VisDroneDataset(val_img_dir, val_anno_dir, transforms=val_transforms)\n",
    "\n",
    "    # Define data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "    # Load pre-trained Faster R-CNN model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    num_classes = len(train_dataset.classes) # 11 classes (including background)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.AdamW(params, lr=0.0001, weight_decay=0.0005)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train for one epoch\n",
    "        metric_logger = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        coco_evaluator = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch} Validation AP: {coco_evaluator.coco_eval['bbox'].stats[0]:.3f}\")\n",
    "\n",
    "        # Save checkpoint (optional)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "        }, f'fasterrcnn_visdrone_epoch_{epoch}.pth')\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Helper functions for distributed training (even if not using)\n",
    "    import sys\n",
    "    import torch.utils.data\n",
    "    import utils\n",
    "\n",
    "    def init_distributed():\n",
    "        pass\n",
    "\n",
    "    def cleanup_distributed():\n",
    "        pass\n",
    "\n",
    "    def get_world_size():\n",
    "        return 1\n",
    "\n",
    "    def is_main_process():\n",
    "        return True\n",
    "\n",
    "    def get_rank():\n",
    "        return 0\n",
    "\n",
    "    utils.init_distributed = init_distributed\n",
    "    utils.cleanup_distributed = cleanup_distributed\n",
    "    utils.get_world_size = get_world_size\n",
    "    utils.is_main_process = is_main_process\n",
    "    utils.get_rank = get_rank\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056939e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "757efcbc",
   "metadata": {},
   "source": [
    "# Running Inference on a Sample of Images\n",
    "Here we take a portion of the images to posttrain the model that has already been pretrained on the COCO dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference of the sample image\n",
    "def generate_predicitons(model, dataset, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        img, target, filename = dataset[i]\n",
    "        img = img.to(device)\n",
    "        imgID = int(target[\"image_id\"].item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model([img])[0]\n",
    "            \n",
    "        for box, label, score in zip(prediction[\"boxes\"], prediction[\"labels\"], prediction[\"scores\"]):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            results.append({\n",
    "                \"image_id\": imgID,\n",
    "                \"category_id\": int(label),\n",
    "                \"bbox\": [float(x1), float(y2), float(x2-x1), float(y2-y1)],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "        \n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75672dc1",
   "metadata": {},
   "source": [
    "# Evaluate with COCO Metrics\n",
    "The metrics used to evaluate the results of the papers submitted in 2019 were different thresholds of Average Precision and Average Recall. First we have to convert the predictions made and the target annotations into COCO format instead of the format used in the VisDrone set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c7f8b",
   "metadata": {},
   "source": [
    "### Convert to COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToCOCO(dataset):\n",
    "    coco = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": i+1, \"name\": str(i+1)} for i in range(10)]\n",
    "    }\n",
    "    \n",
    "    annotationID = 1\n",
    "    for i in range(len(dataset)):\n",
    "        _, target = dataset[i]\n",
    "        imgID = int(target[\"image_id\"].item())\n",
    "        \n",
    "        coco[\"images\"].append({\n",
    "            \"id\": imgID,\n",
    "            \"file_name\": dataset.image_files[imgID]\n",
    "        })\n",
    "        \n",
    "        \n",
    "        for j in range(len(target[\"boxes\"])):\n",
    "            x1, y1, x2, y2 = target[\"boxes\"][i].tolist()\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": annotationID,\n",
    "                \"image_id\": imgID,\n",
    "                \"category_id\": int(target[\"labels\"][i]),\n",
    "                \"bbox\": [float(x1), float(y1), float(x2-x1), float(y2-y1)],\n",
    "                \"area\": float((x2-x1) * (y2-y1)),\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            \n",
    "            annotationID += 1\n",
    "    \n",
    "    return coco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17ef08",
   "metadata": {},
   "source": [
    "### Evaluate Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions\n",
    "predictions = generate_predicitons(model, valDataset, device)\n",
    "print(f\"Saving {len(predictions)} predictions\")\n",
    "with open(\"converted_model_predictions.json\", \"w\") as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "# Generate the convertion to COCO form\n",
    "converted = convertToCOCO(valDataset)\n",
    "print(f\"Saving {len(converted)} predictions\")\n",
    "with open(\"converted_visdrone_annotations.json\", \"w\") as f:\n",
    "    json.dump(converted, f)\n",
    "    \n",
    "# dump json objects into file\n",
    "annotationJSON = \"converted_visdrone_annotations.json\"\n",
    "predictJSON = \"converted_model_predictions.json\"\n",
    "\n",
    "cocoGT = COCO(annotationJSON)\n",
    "cocoDT = cocoGT.loadRes(predictJSON)\n",
    "\n",
    "cocoEval = COCOeval(cocoGT, cocoDT, iouType=\"bbox\")\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ffa43",
   "metadata": {},
   "source": [
    "# Test with test set\n",
    "Asses the performance of the model on a test set. Using mean average precision for evlaution metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480dcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(test_dataloader, desc='Evaluating'):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Process the outputs and targets to calculate evaluation metrics\n",
    "        # This part will depend on the specific evaluation metrics you want to use\n",
    "        # and might require converting the output format.\n",
    "        # Example (simplified - you'll need more detailed processing):\n",
    "        for i, output in enumerate(outputs):\n",
    "            boxes = output['boxes'].cpu().numpy()\n",
    "            scores = output['scores'].cpu().numpy()\n",
    "            labels = output['labels'].cpu().numpy()\n",
    "            # Store these predictions and the corresponding ground truth targets\n",
    "\n",
    "            # Example of storing (you'll need to adapt this):\n",
    "            all_preds.append({'boxes': boxes, 'scores': scores, 'labels': labels})\n",
    "            all_targets.append({'boxes': targets[i]['boxes'].cpu().numpy(), 'labels': targets[i]['labels'].cpu().numpy()})\n",
    "\n",
    "# Calculate evaluation metrics (e.g., mAP) using all_preds and all_targets\n",
    "# This often involves using libraries like pycocotools if your data format aligns with COCO.\n",
    "\n",
    "print(\"Evaluation finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
