{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073f64cf",
   "metadata": {},
   "source": [
    "# General Approach and Requirements\n",
    "\n",
    "The highest scoring model with the smallest required hardware to train and run was the FPN2 model [A11], MMN [A9] or MFaster-RCNN[A.14]. These all required a GTX 1080 TI, which can actually be run on a few froup member's hardware. I will be experimenting with the MFaster-RCNN detection architecture with an FPN integrated to handle the different scales of objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e38489",
   "metadata": {},
   "source": [
    "# MFaster-RCNN with FPN2\n",
    "Following are the packages that we need for the MFaster architecture with an FPN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0575d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CUDA_LAUNCH_BLOCKING=1\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.transforms as T\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch.utils\n",
    "from collections import defaultdict, deque\n",
    "import time, datetime\n",
    "import sys\n",
    "import utils\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320bb728",
   "metadata": {},
   "source": [
    "### Helper objects for result printing and result processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2521c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide a smoothed version over a\n",
    "    window of size `window_size`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count if self.count > 0 else 0\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value,\n",
    "        )\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"  \"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{attr}'\")\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(f\"{name}: {str(meter)}\")\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = \"\"\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
    "        data_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
    "        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n",
    "        if sys.platform == \"win32\":\n",
    "            log_msg = header + \"[{0\" + space_fmt + \"}/{1}] eta: {eta} {meters}\"\n",
    "        else:\n",
    "            log_msg = header + \"[{0\" + space_fmt + \"}/{1}] eta: {eta} {meters}\"\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i - 1)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(\n",
    "                        log_msg.format(\n",
    "                            i,\n",
    "                            len(iterable),\n",
    "                            eta=eta_string,\n",
    "                            meters=str(self),\n",
    "                            mem=\"{:.0f}M\".format(torch.cuda.max_memory_allocated() / MB),\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    print(log_msg.format(i, len(iterable), eta=eta_string, meters=str(self)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print(f\"{header} Total time: {total_time_str} ({total_time / len(iterable):.4f} s / it)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83167f5",
   "metadata": {},
   "source": [
    "# Capture the Dataset for Preprocessing\n",
    "Read in all of the image files into a list, as well as their corresponding annotations. The dataset also has a getter for the lenght of the dataset for iterative purposes later along with a getitem function for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ae7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisDroneDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.image_files = sorted(os.listdir(root_dir))\n",
    "        self.transforms = transforms\n",
    "        self.annotations = self._load_annotations()\n",
    "        self.classes = ['background', 'predestrian', 'people', 'car', 'van', 'bus', 'truck', 'tricycle', 'awning-tricycle', 'bicycle', 'motorcycle']\n",
    "\n",
    "    def _load_annotations(self):\n",
    "        annotations = {}\n",
    "        for imgName in self.image_files:\n",
    "            annotation_name = imgName.replace('.jpg', '.txt')\n",
    "            annotation_path = os.path.join(self.annotation_dir, annotation_name)\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            if os.path.exists(annotation_path):\n",
    "                with open(annotation_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            x, y, w, h, score, categoryID, truncation, occlusion = map(int, line.strip().split(',')[:8])\n",
    "\n",
    "                            if 1 <= categoryID <= 10:\n",
    "                                boxes.append([x, y, x+w, y+h])\n",
    "                                labels.append(categoryID)\n",
    "                            \n",
    "                        except ValueError as e:\n",
    "                            print(f\"Error parsing line in {annotation_path}: {line.strip()} - {e}\")\n",
    "            annotations[imgName] = {'boxes': boxes, 'labels': labels}\n",
    "            \n",
    "        return annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgName = self.image_files[idx]\n",
    "        imgPath = os.path.join(self.root_dir, imgName)\n",
    "        annotationData = self.annotations[imgName]\n",
    "\n",
    "        img = Image.open(imgPath).convert(\"RGB\")\n",
    "        boxes = torch.as_tensor(annotationData['boxes'], dtype=torch.float32) \n",
    "        labels = torch.as_tensor(annotationData['labels'], dtype=torch.int64) \n",
    "        \n",
    "        # with open(annotation_path, 'r') as f:\n",
    "        #     for line in f:\n",
    "        #         x, y, w, h, score, categoryID, truncation, occlusion = map(int, line.strip().split(',')[:8]) # Assuming standard VisDrone annotation format\n",
    "        #         if not (1 < categoryID <= 10):\n",
    "        #             continue\n",
    "        #         boxes.append([x, y, x+w, y+h])\n",
    "        #         labels.append(categoryID + 1) # Assuming the 6th value is the class label\n",
    "\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"iamge_id\": torch.tensor([idx])\n",
    "        }\n",
    "        \n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082c6ac",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Resize and normalize the dataset with flips and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc9578f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50688c22",
   "metadata": {},
   "source": [
    "# Load the dataset with dataloaders\n",
    "Create custom dataset with the augmented data appended to the original dataset using the data loaders from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e5949ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "631e6b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, dataLoader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    metricLogger = MetricLogger(delimiter=\" \")\n",
    "    metricLogger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = f'Epoch: [{epoch}]'\n",
    "    \n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmupFactor = 1./100\n",
    "        warmupIter = min(len(dataLoader) - 1, 100)\n",
    "        \n",
    "        def lr_lambda(step):\n",
    "            if step < warmupIter:\n",
    "                alpha = float(step) / warmupIter\n",
    "                return warmupFactor * (1 - alpha) + alpha\n",
    "            return 1\n",
    "        \n",
    "        lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "        \n",
    "    for i, (images, targets) in enumerate(metricLogger.log_every(dataLoader, print_freq, header)):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "        lossDict = model(images, targets)\n",
    "        losses = sum(loss for loss in lossDict.values())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "        metricLogger.update(loss=losses, **lossDict)\n",
    "        metricLogger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "    return metricLogger\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3e63d",
   "metadata": {},
   "source": [
    "# Make Results in COCO format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d4a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_coco_results(cocoGt, predictions, iouType): \n",
    "    results = []\n",
    "    for image_id, prediction in predictions.items():\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "\n",
    "        boxes = prediction[\"boxes\"].tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        cocoPredictions = []\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            cocoPredictions.append(\n",
    "                {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"bbox\": [box[0], box[1], box[2] - box[0], box[3] - box[1]], # Convert to [x, y, w, h]\n",
    "                    \"score\": score,\n",
    "                    \"category_id\": int(label),\n",
    "                }\n",
    "            )\n",
    "        results.extend(cocoPredictions)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def convertToCOCO(dataset):\n",
    "    coco = COCO()\n",
    "    coco.dataset = {\n",
    "        \"info\": {\"description\": \"VisDrone 2019 Dataset\"},\n",
    "        \"licenses\": [{\"id\": 1, \"name\": \"Unknown\", \"url\": \"\"}],\n",
    "        \"categories\": [{\"id\": i + 1, \"name\": name} for i, name in enumerate(dataset.classes[1:])], # Exclude 'ignored region'\n",
    "        \"images\": [],\n",
    "        \"annotations\": []\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "    for i in range(len(dataset)):\n",
    "        img_name = dataset.image_files[i]\n",
    "        _, target = dataset[i]\n",
    "        image_info = {\"id\": i, \"file_name\": img_name, \"width\": 0, \"height\": 0} # Actual width and height are not used in evaluation\n",
    "        coco.dataset[\"images\"].append(image_info)\n",
    "\n",
    "        for j in range(len(target[\"boxes\"])):\n",
    "            bbox = target[\"boxes\"][j].tolist()\n",
    "            label = target[\"labels\"][j].item()\n",
    "            annotation = {\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": i,\n",
    "                \"category_id\": label,\n",
    "                \"bbox\": [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]], # Convert to [x, y, w, h]\n",
    "                \"area\": (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]),\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            coco.dataset[\"annotations\"].append(annotation)\n",
    "            annotation_id += 1\n",
    "\n",
    "    coco.createIndex()\n",
    "    return coco\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c3f58",
   "metadata": {},
   "source": [
    "# Evaluate COCO Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbc0450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    metricLogger = MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "    \n",
    "    coco = convertToCOCO(data_loader.dataset)\n",
    "    iou_types = [\"bbox\"]\n",
    "    cocoEvaluator = cocoEvaluator(coco, iou_types)\n",
    "    \n",
    "    for images, targets in metricLogger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in outputs]\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        cocoEvaluator.update(res)\n",
    "        \n",
    "    cocoEvaluator.synchronize_between_processes()\n",
    "    \n",
    "    cocoEvaluator.accumulate()\n",
    "    cocoEvaluator.summarize()\n",
    "    \n",
    "    return cocoEvaluator\n",
    "\n",
    "class CocoEvaluator:\n",
    "    def __init__(self, cocoGt, iouTypes):\n",
    "        if not isinstance(iouTypes, (list, tuple)):\n",
    "            raise TypeError(f\"iou_types must be a list or tuple of strings, got {iouTypes}\")\n",
    "        allowed_iou_types = (\"bbox\", \"segm\")\n",
    "        for iou_type in iouTypes:\n",
    "            if iou_type not in allowed_iou_types:\n",
    "                raise ValueError(f\"iou_type: {iou_type} not in {allowed_iou_types}\")\n",
    "        self.cocoGt = cocoGt\n",
    "        self.iouTypes = iouTypes\n",
    "        self.cocoEval = {}\n",
    "        self.imgIds = []\n",
    "        \n",
    "    def update(self, predictions):\n",
    "        imgIds = list(np.unique(list(predictions.keys())))\n",
    "        self.imgIds.extend(imgIds)\n",
    "        \n",
    "        for iou_type in self.iouTypes:\n",
    "            if len(self.cocoEval) == 0:\n",
    "                self.cocoEval[iou_type] = COCOeval(self.cocoGt, _create_coco_results(self.coco_gt, predictions, iou_type), iou_type)\n",
    "            else:\n",
    "                cocoDt = _create_coco_results(self.cocoGt, predictions, iou_type)\n",
    "                self.cocoEval[iou_type].cocoDt = self.cocoEval[iou_type].cocoGt.loadRes(cocoDt)\n",
    "            \n",
    "    def synchronize_between_processes(self):\n",
    "        pass\n",
    "    \n",
    "    def accumulate(self):\n",
    "        for coco_eval in self.cocoEval.values():\n",
    "            coco_eval.accumulate()\n",
    "            \n",
    "    def summarize(self):\n",
    "        for iouType, cocoEval in self.coco_eval.items():\n",
    "            print(f\"IoU metric: {iouType}\")\n",
    "            cocoEval.summarize()\n",
    "            \n",
    "    @property\n",
    "    def results(self):\n",
    "        return {iouType: cocoEval.stats.tolist() for iouType, cocoEval in self.cocoEval.items()}\n",
    "            \n",
    "     \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329a287",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33301708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/COMP9444/VisDroneInterpretaion/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/root/COMP9444/VisDroneInterpretaion/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/COMP9444/VisDroneInterpretaion/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_192097/2917315702.py\", line 61, in __getitem__\n    img, target = self.transforms(img)\nValueError: too many values to unpack (expected 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m utils\u001b[38;5;241m.\u001b[39mis_main_process \u001b[38;5;241m=\u001b[39m is_main_process\n\u001b[1;32m     89\u001b[0m utils\u001b[38;5;241m.\u001b[39mget_rank \u001b[38;5;241m=\u001b[39m get_rank\n\u001b[0;32m---> 91\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     metricLogger \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Evaluate on the validation set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, dataLoader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m     lr_scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLambdaLR(optimizer, lr_lambda)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metricLogger\u001b[38;5;241m.\u001b[39mlog_every(dataLoader, print_freq, header)):\n\u001b[1;32m     21\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m     22\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "Cell \u001b[0;32mIn[2], line 92\u001b[0m, in \u001b[0;36mMetricLogger.log_every\u001b[0;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[1;32m     90\u001b[0m     log_msg \u001b[38;5;241m=\u001b[39m header \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m space_fmt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}/\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m] eta: \u001b[39m\u001b[38;5;132;01m{eta}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{meters}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m MB \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024.0\u001b[39m\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m     93\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/COMP9444/VisDroneInterpretaion/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/COMP9444/VisDroneInterpretaion/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/COMP9444/VisDroneInterpretaion/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/COMP9444/VisDroneInterpretaion/myenv/lib/python3.10/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/COMP9444/VisDroneInterpretaion/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/root/COMP9444/VisDroneInterpretaion/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/COMP9444/VisDroneInterpretaion/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_192097/2917315702.py\", line 61, in __getitem__\n    img, target = self.transforms(img)\nValueError: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define paths\n",
    "    train_img_dir = \"VisDrone2019-DET-train/images\"\n",
    "    train_anno_dir = \"VisDrone2019-DET-train/annotations\"\n",
    "    val_img_dir = \"images\"\n",
    "    val_anno_dir = \"annotations\"\n",
    "\n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(train_img_dir) or not os.path.exists(train_anno_dir) or not os.path.exists(val_img_dir) or not os.path.exists(val_anno_dir):\n",
    "        print(\"Error: One or more image or annotation directories not found. Please adjust the paths.\")\n",
    "        return\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Define transforms\n",
    "    train_transforms = get_transform(train=True)\n",
    "    val_transforms = get_transform(train=False)\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = VisDroneDataset(train_img_dir, train_anno_dir, transforms=train_transforms)\n",
    "    val_dataset = VisDroneDataset(val_img_dir, val_anno_dir, transforms=val_transforms)\n",
    "\n",
    "    # Define data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, collate_fn=splitData)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=splitData)\n",
    "\n",
    "    # Load pre-trained Faster R-CNN model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    num_classes = len(train_dataset.classes) # 11 classes (including background)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.AdamW(params, lr=0.0001, weight_decay=0.0005)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train for one epoch\n",
    "        metricLogger = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        cocoEvaluator = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch} Validation AP: {cocoEvaluator.coco_eval['bbox'].stats[0]:.3f}\")\n",
    "\n",
    "        # Save checkpoint (optional)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "        }, f'fasterrcnn_visdrone_epoch_{epoch}.pth')\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Helper functions for distributed training (even if not using)\n",
    "    import sys\n",
    "    import torch.utils.data\n",
    "    import utils\n",
    "\n",
    "    def init_distributed():\n",
    "        pass\n",
    "\n",
    "    def cleanup_distributed():\n",
    "        pass\n",
    "\n",
    "    def get_world_size():\n",
    "        return 1\n",
    "\n",
    "    def is_main_process():\n",
    "        return True\n",
    "\n",
    "    def get_rank():\n",
    "        return 0\n",
    "\n",
    "    utils.init_distributed = init_distributed\n",
    "    utils.cleanup_distributed = cleanup_distributed\n",
    "    utils.get_world_size = get_world_size\n",
    "    utils.is_main_process = is_main_process\n",
    "    utils.get_rank = get_rank\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056939e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "757efcbc",
   "metadata": {},
   "source": [
    "# Running Inference on a Sample of Images\n",
    "Here we take a portion of the images to posttrain the model that has already been pretrained on the COCO dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference of the sample image\n",
    "def generate_predicitons(model, dataset, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        img, target, filename = dataset[i]\n",
    "        img = img.to(device)\n",
    "        imgID = int(target[\"image_id\"].item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model([img])[0]\n",
    "            \n",
    "        for box, label, score in zip(prediction[\"boxes\"], prediction[\"labels\"], prediction[\"scores\"]):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            results.append({\n",
    "                \"image_id\": imgID,\n",
    "                \"category_id\": int(label),\n",
    "                \"bbox\": [float(x1), float(y2), float(x2-x1), float(y2-y1)],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "        \n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75672dc1",
   "metadata": {},
   "source": [
    "# Evaluate with COCO Metrics\n",
    "The metrics used to evaluate the results of the papers submitted in 2019 were different thresholds of Average Precision and Average Recall. First we have to convert the predictions made and the target annotations into COCO format instead of the format used in the VisDrone set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c7f8b",
   "metadata": {},
   "source": [
    "### Convert to COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToCOCO(dataset):\n",
    "    coco = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": i+1, \"name\": str(i+1)} for i in range(10)]\n",
    "    }\n",
    "    \n",
    "    annotationID = 1\n",
    "    for i in range(len(dataset)):\n",
    "        _, target = dataset[i]\n",
    "        imgID = int(target[\"image_id\"].item())\n",
    "        \n",
    "        coco[\"images\"].append({\n",
    "            \"id\": imgID,\n",
    "            \"file_name\": dataset.image_files[imgID]\n",
    "        })\n",
    "        \n",
    "        \n",
    "        for j in range(len(target[\"boxes\"])):\n",
    "            x1, y1, x2, y2 = target[\"boxes\"][i].tolist()\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": annotationID,\n",
    "                \"image_id\": imgID,\n",
    "                \"category_id\": int(target[\"labels\"][i]),\n",
    "                \"bbox\": [float(x1), float(y1), float(x2-x1), float(y2-y1)],\n",
    "                \"area\": float((x2-x1) * (y2-y1)),\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            \n",
    "            annotationID += 1\n",
    "    \n",
    "    return coco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17ef08",
   "metadata": {},
   "source": [
    "### Evaluate Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions\n",
    "predictions = generate_predicitons(model, valDataset, device)\n",
    "print(f\"Saving {len(predictions)} predictions\")\n",
    "with open(\"converted_model_predictions.json\", \"w\") as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "# Generate the convertion to COCO form\n",
    "converted = convertToCOCO(valDataset)\n",
    "print(f\"Saving {len(converted)} predictions\")\n",
    "with open(\"converted_visdrone_annotations.json\", \"w\") as f:\n",
    "    json.dump(converted, f)\n",
    "    \n",
    "# dump json objects into file\n",
    "annotationJSON = \"converted_visdrone_annotations.json\"\n",
    "predictJSON = \"converted_model_predictions.json\"\n",
    "\n",
    "cocoGT = COCO(annotationJSON)\n",
    "cocoDT = cocoGT.loadRes(predictJSON)\n",
    "\n",
    "cocoEval = COCOeval(cocoGT, cocoDT, iouType=\"bbox\")\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ffa43",
   "metadata": {},
   "source": [
    "# Test with test set\n",
    "Asses the performance of the model on a test set. Using mean average precision for evlaution metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480dcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(test_dataloader, desc='Evaluating'):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Process the outputs and targets to calculate evaluation metrics\n",
    "        # This part will depend on the specific evaluation metrics you want to use\n",
    "        # and might require converting the output format.\n",
    "        # Example (simplified - you'll need more detailed processing):\n",
    "        for i, output in enumerate(outputs):\n",
    "            boxes = output['boxes'].cpu().numpy()\n",
    "            scores = output['scores'].cpu().numpy()\n",
    "            labels = output['labels'].cpu().numpy()\n",
    "            # Store these predictions and the corresponding ground truth targets\n",
    "\n",
    "            # Example of storing (you'll need to adapt this):\n",
    "            all_preds.append({'boxes': boxes, 'scores': scores, 'labels': labels})\n",
    "            all_targets.append({'boxes': targets[i]['boxes'].cpu().numpy(), 'labels': targets[i]['labels'].cpu().numpy()})\n",
    "\n",
    "# Calculate evaluation metrics (e.g., mAP) using all_preds and all_targets\n",
    "# This often involves using libraries like pycocotools if your data format aligns with COCO.\n",
    "\n",
    "print(\"Evaluation finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
